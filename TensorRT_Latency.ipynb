{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-sEPq-3xlSZ",
        "outputId": "6769c42d-fbf6-4312-881b-9fc54ee1ede8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "Fri Dec 27 06:08:47 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 257 kB in 1s (227 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libnvinfer8 is already the newest version (8.6.1.6-1+cuda12.0).\n",
            "libnvinfer-plugin8 is already the newest version (8.6.1.6-1+cuda12.0).\n",
            "libnvinfer-dev is already the newest version (10.7.0.23-1+cuda12.6).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 50 not upgraded.\n",
            "fatal: destination path 'tensorflow' already exists and is not an empty directory.\n",
            "/content/tensorflow\n",
            "Already on 'r2.10'\n",
            "Your branch is up to date with 'origin/r2.10'.\n",
            "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
            "/bin/bash: -c: line 1: `bazel build --config=cuda --config=monolithic ... (Specify the build target with TensorRT support)'\n",
            "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
            "/bin/bash: -c: line 1: `bazel install ... (Install the built TensorFlow package)'\n"
          ]
        }
      ],
      "source": [
        "# Ensure CUDA and cuDNN are installed\n",
        "!nvcc --version\n",
        "!nvidia-smi\n",
        "# Install the required dependencies for building TensorFlow with TensorRT support\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y libnvinfer8 libnvinfer-dev libnvinfer-plugin8\n",
        "# (Install other necessary packages as mentioned in TensorFlow documentation)\n",
        "# Clone the TensorFlow repository and checkout the desired branch\n",
        "!git clone https://github.com/tensorflow/tensorflow.git\n",
        "%cd tensorflow\n",
        "!git checkout r2.10 # Check the TensorFlow-TensorRT compatibility matrix for the correct branch.\n",
        "# Configure TensorFlow build with TensorRT enabled\n",
        "# ./configure\n",
        "# (During configuration, enable TensorRT support when prompted)\n",
        "# If you are using a virtual environment, activate it before building TensorFlow.\n",
        "# Build and install TensorFlow\n",
        "!bazel build --config=cuda --config=monolithic ... (Specify the build target with TensorRT support)\n",
        "!bazel install ... (Install the built TensorFlow package)\n",
        "# After successful installation, restart the runtime to ensure the new TensorFlow installation is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dvcSHgfz8m9",
        "outputId": "f005fbda-d4cf-49eb-eb4e-df2a7238ef9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Dec 27 06:08:59 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwutDdg51rgy",
        "outputId": "2cde6c24-5e3d-47b4-dffd-299fbe70fc67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tf2onnx in /usr/local/lib/python3.10/dist-packages (1.16.1)\n",
            "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (1.26.4)\n",
            "Requirement already satisfied: onnx>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (1.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (1.17.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (24.3.25)\n",
            "Requirement already satisfied: protobuf~=3.20 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (3.20.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "!pip install tf2onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNXWqRfj3yct",
        "outputId": "f0474be0-c18b-4554-c499-79ebc28488ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root\n"
          ]
        }
      ],
      "source": [
        "%cd ~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YsAveA1Z0b14"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JVkXuSvF4HIX"
      },
      "outputs": [],
      "source": [
        "def convert_keras_to_tensorrt(keras_model_path, trt_model_dir):\n",
        "    \"\"\"\n",
        "    Converts a Keras model to a TensorRT-optimized SavedModel.\n",
        "\n",
        "    Args:\n",
        "        keras_model_path (str): Path to the .keras model file.\n",
        "        trt_model_dir (str): Directory where the TensorRT-optimized model will be saved.\n",
        "    \"\"\"\n",
        "    print(\"Loading Keras model...\")\n",
        "    # Load the Keras model\n",
        "    keras_model = tf.keras.models.load_model(keras_model_path)\n",
        "\n",
        "    # Create a temporary directory to save the SavedModel\n",
        "    temp_saved_model_dir = \"temp_saved_model\"\n",
        "    os.makedirs(temp_saved_model_dir, exist_ok=True)\n",
        "\n",
        "    # Save the Keras model as a SavedModel\n",
        "    print(\"Saving Keras model as SavedModel...\")\n",
        "    tf.saved_model.save(keras_model, temp_saved_model_dir)\n",
        "\n",
        "    print(\"Converting Keras model to TensorRT...\")\n",
        "    # Initialize the TensorRT converter\n",
        "    params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n",
        "        precision_mode=trt.TrtPrecisionMode.FP16,  # Use FP16 for faster inference (if supported)\n",
        "        max_workspace_size_bytes=1 << 30          # 1GB workspace size\n",
        "    )\n",
        "    # Pass the temporary SavedModel directory to the converter\n",
        "    converter = trt.TrtGraphConverterV2(input_saved_model_dir=temp_saved_model_dir, conversion_params=params)\n",
        "\n",
        "    # Convert the Keras model\n",
        "    converter.convert()\n",
        "\n",
        "    # Save the TensorRT-optimized model\n",
        "    print(f\"Saving TensorRT-optimized model to {trt_model_dir}...\")\n",
        "    converter.save(trt_model_dir)\n",
        "    print(f\"TensorRT-optimized model saved at {trt_model_dir}\")\n",
        "\n",
        "    # Optionally remove the temporary SavedModel directory\n",
        "    # import shutil\n",
        "    # shutil.rmtree(temp_saved_model_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "O6diCWd74TF3"
      },
      "outputs": [],
      "source": [
        "def load_tensorrt_model_and_infer(trt_model_dir, input_data):\n",
        "    \"\"\"\n",
        "    Loads a TensorRT-optimized model and performs inference.\n",
        "\n",
        "    Args:\n",
        "        trt_model_dir (str): Directory of the TensorRT-optimized model.\n",
        "        input_data (numpy.ndarray): Input data for inference.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Model predictions.\n",
        "    \"\"\"\n",
        "    print(\"Loading TensorRT-optimized model...\")\n",
        "    trt_model = tf.saved_model.load(trt_model_dir)\n",
        "    infer = trt_model.signatures[\"serving_default\"]\n",
        "\n",
        "    print(\"Running inference...\")\n",
        "    # Perform inference\n",
        "    predictions = infer(tf.convert_to_tensor(input_data))\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vCv6ykd64Wa4"
      },
      "outputs": [],
      "source": [
        "keras_model_path = \"/content/drive/MyDrive/MCW/ASSIGNMENT-1/DataAgumentedModel.keras\"         # Path to your .keras model\n",
        "trt_model_dir = \"/content/drive/MyDrive/MCW/ASSIGNMENT-1/model_trt\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34fhowwMvELQ",
        "outputId": "f7626d98-47a5-4bda-c2db-b35f951ad724"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BH8lLzz4eeZ",
        "outputId": "47db5965-bacb-414e-c01c-b34907672bf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Keras model...\n",
            "Saving Keras model as SavedModel...\n",
            "Converting Keras model to TensorRT...\n",
            "Saving TensorRT-optimized model to /content/drive/MyDrive/MCW/ASSIGNMENT-1/model_trt...\n",
            "TensorRT-optimized model saved at /content/drive/MyDrive/MCW/ASSIGNMENT-1/model_trt\n"
          ]
        }
      ],
      "source": [
        "convert_keras_to_tensorrt(keras_model_path, trt_model_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model"
      ],
      "metadata": {
        "id": "11vUCPsHv9c1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Original model\n",
        "original_model = load_model('/content/drive/MyDrive/MCW/ASSIGNMENT-1/DataAgumentedModel.keras')\n",
        "\n",
        "# Save the original model as a SavedModel\n",
        "tf.saved_model.save(original_model, '/content/original_saved_model')\n",
        "\n",
        "# Load the original model as a SavedModel\n",
        "original_saved_model = tf.saved_model.load('/content/original_saved_model')\n",
        "\n",
        "# Now you can access the signatures\n",
        "original_serving_fn = original_saved_model.signatures['serving_default']\n",
        "\n",
        "# TensorRT-optimized model (no changes needed)\n",
        "optimized_model = tf.saved_model.load('/content/drive/MyDrive/MCW/ASSIGNMENT-1/model_trt')\n",
        "optimized_serving_fn = optimized_model.signatures['serving_default']"
      ],
      "metadata": {
        "id": "OcGSLjhlwCDw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sample batch of data for benchmarking\n",
        "input_shape = (32, 32, 3)\n",
        "batch_size = 32\n",
        "dummy_input = np.random.random((batch_size, *input_shape)).astype(np.float32)\n",
        "original_input = tf.convert_to_tensor(dummy_input)\n",
        "optimized_input = tf.convert_to_tensor(dummy_input)"
      ],
      "metadata": {
        "id": "5g42Jp0-wDn0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time  # Import the 'time' module\n",
        "\n",
        "def measure_latency(model_fn, input_key, input_tensor, warmup=10, iterations=100):\n",
        "    \"\"\"\n",
        "    Measures the average latency of a TensorFlow model.\n",
        "\n",
        "    Args:\n",
        "        model_fn: The model function to be measured.\n",
        "        input_key (str): The key used to pass the input tensor to the model function.\n",
        "                           Should be 'inputs' for SavedModel signatures.\n",
        "        input_tensor: The input tensor to be used for inference.\n",
        "        warmup (int): The number of warm-up runs to perform before measuring latency.\n",
        "        iterations (int): The number of iterations to run for latency measurement.\n",
        "\n",
        "    Returns:\n",
        "        float: The average latency in milliseconds.\n",
        "    \"\"\"\n",
        "\n",
        "    # Warm-up runs\n",
        "    for _ in range(warmup):\n",
        "        _ = model_fn(**{input_key: input_tensor})  # Pass input using the correct key\n",
        "\n",
        "    # Measure latency\n",
        "    start_time = time.time()\n",
        "    for _ in range(iterations):\n",
        "        _ = model_fn(**{input_key: input_tensor})  # Pass input using the correct key\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Calculate average latency\n",
        "    avg_latency = (end_time - start_time) / iterations\n",
        "    return avg_latency * 1000  # Convert to milliseconds"
      ],
      "metadata": {
        "id": "mjN93Wu9wFhL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_latency = measure_latency(\n",
        "    original_serving_fn, \"inputs\", original_input # Changed input_key to 'inputs'\n",
        ")"
      ],
      "metadata": {
        "id": "BA_0IRYowG2V"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimized_latency = measure_latency(\n",
        "    optimized_serving_fn, \"inputs\", optimized_input\n",
        ")"
      ],
      "metadata": {
        "id": "NBFrE8vQwISS"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Original Model Latency: {original_latency:.2f} ms\")\n",
        "print(f\"Optimized Model Latency: {optimized_latency:.2f} ms\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQ9aXODZwJ18",
        "outputId": "0bcc8b15-e9ac-461a-d015-ad8694a59ed5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Model Latency: 2.57 ms\n",
            "Optimized Model Latency: 0.53 ms\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}